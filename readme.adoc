= NATS-AB

Load test platform which terraform based. We try to deploy NATS JS cluster with Leaf protocol and measure rpc bandwidth.

== Manuals
* https://github.com/nats-io/jetstream-leaf-nodes-demo
* https://www.youtube.com/watch?v=0MkS_S7lyHk


==
[source]
----
nats-server -c cluster-hub.conf
----

== outpus
.context_sys
create context in nats, contains all servers

.context_hub
create context in nats, contain hub server

.context_spoke_1
create context in nats, contains spoke-1 server


=== test
install natscli

for AWS Linux 2:
[source, bash]
----
curl -L https://github.com/nats-io/natscli/releases/download/v0.0.29/nats-0.0.29-linux-amd64.zip -o natscli.zip
unzip natscli.zip -d natscli
cp natscli/nats-0.0.29-linux-amd64/nats /usr/bin
----

add contexts (from output), example:
[source]
----
nats context save sys --server "nats://172.31.20.65:4222,nats://172.31.29.145:4222,nats://172.31.30.204:4222,nats://172.31.21.18:4222"

 nats context save spoke-1 --server "nats://172.31.30.204:4222,nats://172.31.21.18:4222"

nats context save hub --server "nats://172.31.20.65:4222,nats://172.31.29.145:4222,"
----

check out
[source,bash]
----
nats --context=sys  server list
----

looks like:
[source,bash]
----
╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│                                                     Server Overview                                                      │
├─────────────┬─────────────┬───────────┬─────────┬─────┬───────┬──────┬────────┬─────┬────────┬─────┬──────┬────────┬─────┤
│ Name        │ Cluster     │ IP        │ Version │ JS  │ Conns │ Subs │ Routes │ GWs │ Mem    │ CPU │ Slow │ Uptime │ RTT │
├─────────────┼─────────────┼───────────┼─────────┼─────┼───────┼──────┼────────┼─────┼────────┼─────┼──────┼────────┼─────┤
│ srv-leaf158 │ leaf        │ 0.0.0.0   │ 2.7.2   │ yes │ 1     │ 200  │ 1      │ 0   │ 10 MiB │ 0.0 │ 0    │ 2.22s  │ 1ms │
│ srv-hub228  │ cluster-hub │ 0.0.0.0   │ 2.7.2   │ yes │ 0     │ 152  │ 1      │ 0   │ 11 MiB │ 0.0 │ 0    │ 20.86s │ 1ms │
│ srv-leaf162 │ leaf        │ 0.0.0.0   │ 2.7.2   │ yes │ 0     │ 200  │ 1      │ 0   │ 11 MiB │ 0.0 │ 0    │ 9.16s  │ 1ms │
│ srv-hub101  │ cluster-hub │ 0.0.0.0   │ 2.7.2   │ yes │ 0     │ 152  │ 1      │ 0   │ 12 MiB │ 0.0 │ 0    │ 18.90s │ 2ms │
├─────────────┼─────────────┼───────────┼─────────┼─────┼───────┼──────┼────────┼─────┼────────┼─────┼──────┼────────┼─────┤
│             │ 2 Clusters  │ 4 Servers │         │ 4   │ 1     │ 704  │        │     │ 44 MiB │     │ 0    │        │     │
╰─────────────┴─────────────┴───────────┴─────────┴─────┴───────┴──────┴────────┴─────┴────────┴─────┴──────┴────────┴─────╯

╭────────────────────────────────────────────────────────────────────────────────╮
│                                Cluster Overview                                │
├─────────────┬────────────┬───────────────────┬───────────────────┬─────────────┤
│ Cluster     │ Node Count │ Outgoing Gateways │ Incoming Gateways │ Connections │
├─────────────┼────────────┼───────────────────┼───────────────────┼─────────────┤
│ cluster-hub │ 2          │ 0                 │ 0                 │ 0           │
│ leaf        │ 2          │ 0                 │ 0                 │ 1           │
├─────────────┼────────────┼───────────────────┼───────────────────┼─────────────┤
│             │ 4          │ 0                 │ 0                 │ 1           │
╰─────────────┴────────────┴───────────────────┴───────────────────┴─────────────╯
----


=== compliance
Check credentials test user which uses in context: `sys`, `hub` and `spoke-1`

should show JS limits:
[source,bash]
nats account info --context sys

js info:
[source,bash]
nats  --context sys  server report jetstream

==== publisher throughput test (hub)
===== A
[source, bash]
nats bench test --context hub  --pub 1 --size 16

===== B
[source, bash]
nats bench test --context hub    --pub 1 --size 16 --msgs 10000000

==== publisher throughput test (spoke-1)
=====  A
[source, bash]
nats bench test --context spoke-1    --pub 1 --size 16

=====  B
[source, bash]
nats bench test --context spoke-1    --pub 1 --size 16 --msgs 10000000

==== publish/subscribe throughput test(hub->hub)
===== A
[source, bash]
nats bench test --context hub    --pub 1 --sub 1 --size 16

===== B
[source, bash]
nats bench test --context hub    --pub 1 --sub 1 --size 16 --msgs 10000000

==== publish/subscribe throughput test(hub->spoke-1)
===== A
[source, bash]
nats bench test --context spoke-1   --sub 1

[source, bash]
nats bench test --context hub   --pub 1 --size 16

===== B
[source, bash]
nats bench test --context spoke-1   --sub 1 --msgs 10000000

[source, bash]
nats bench test --context hub    --pub 1 --size 16 --msgs 10000000

==== publish/subscribe throughput test(spoke-1->hub)
===== A
[source, bash]
nats bench test --context  hub  --sub 1

[source, bash]
nats bench test --context spoke-1   --pub 1 --size 16

===== B
[source, bash]
nats bench test --context hub   --sub 1 --msgs 10000000

[source, bash]
nats bench test --context spoke-1    --pub 1 --size 16 --msgs 10000000

==== request/reply latency test(hub->hub->hub)
===== A
[source, bash]
nats bench test --context  hub  --sub 20  --reply --msgs 1000000 --size 16

[source, bash]
nats bench test --context hub   --pub 20 --request --msgs 1000000 --size 16

===== B
[source, bash]
nats bench test --context  hub  --sub 20  --reply --msgs 1000000 --size 16

[source, bash]
nats bench test --context hub   --pub 100 --request --msgs 1000000 --size 16

==== request/reply latency test(spoke-1->hub->spoke-1)
===== A
[source, bash]
nats bench test --context  hub --sub 20 --reply --msgs 1000000 --size 16

[source, bash]
nats bench test --context spoke-1 --pub 20 --request --msgs 1000000 --size 16

===== B
[source, bash]
nats bench test --context  hub --sub 20 --reply --msgs 1000000 --size 16

[source, bash]
nats bench test --context spoke-1 --pub 100 --request --msgs 1000000 --size 16


==== JetStream publication performance (file)
===== A
[source, bash]
nats bench test --context hub --js --pub 10 --size 16 --msgs 1000000 --storage="file"  --stream="benchstream-file"

===== B
[source, bash]
nats bench test --context spoke-1 --js --pub 10 --size 16 --msgs 1000000 --storage="file" --stream="benchstream-file"

==== JetStream publication performance (memory)
===== A
[source, bash]
nats bench test --context  hub --js --pub 10 --size 16 --msgs 1000000 --storage="memory"

===== B
[source, bash]
nats bench test --context  spoke-1 --js --pub 10 --size 16 --msgs 1000000 --storage="memory"

==== JetStream consumption (replay) performance (memory)
===== A
[source, bash]
nats bench test --context  hub --js --sub 10 --size 16 --msgs 1000000 --storage="memory"

===== B
[source, bash]
nats bench test --context  spoke-1 --js --sub 10 --size 16 --msgs 1000000 --storage="memory"

== Test1

Topology: hub: t3.medium x2 leaf: t3.medium  x2
|===
|test |A |B

|publisher throughput test(hub)
|Pub stats: 6,339,559 msgs/sec ~ 96.73 MB/sec
|Pub stats: 6,129,472 msgs/sec ~ 93.53 MB/sec

|publisher throughput test (spoke-1)
|Pub stats: 6,268,973 msgs/sec ~ 95.66 MB/sec
|Pub stats: 6,184,261 msgs/sec ~ 94.36 MB/sec

|publish/subscribe throughput test(hub->hub)
|NATS Pub/Sub stats: 3,839,529 msgs/sec ~ 58.59 MB/sec,Pub stats: 1,962,664 msgs/sec ~ 29.95 MB/sec, Sub stats: 1,939,357 msgs/sec ~ 29.59 MB/sec
|NATS Pub/Sub stats: 4,072,194 msgs/sec ~ 62.14 MB/sec, Pub stats: 2,197,401 msgs/sec ~ 33.53 MB/sec, Sub stats: 2,037,052 msgs/sec ~ 31.08 MB/sec

|publish/subscribe throughput test(hub->spoke-1)
|Sub stats: 1,090,251 msgs/sec ~ 133.09 MB/sec, Pub stats: 2,209,871 msgs/sec ~ 33.72 MB/sec
|Sub stats: 1,343,449 msgs/sec ~ 164.00 MB/sec, Pub stats: 1,464,809 msgs/sec ~ 22.35 MB/sec

|publish/subscribe throughput test(spoke-1->hub)
|Sub stats: 1,193,965 msgs/sec ~ 145.75 MB/sec, Sub stats: 1,193,965 msgs/sec ~ 145.75 MB/sec
|Sub stats: 1,734,123 msgs/sec ~ 211.68 MB/sec, Pub stats: 1,870,011 msgs/sec ~ 28.53 MB/sec

|request/reply latency test(hub->hub->hub)
|Pub stats: 21,416 msgs/sec ~ 334.64 KB/sec
|

|request/reply latency test(spoke-1->hub->spoke-1)
|Pub stats: 17,229 msgs/sec ~ 269.21 KB/sec
|

|JetStream publication performance (file)
|Pub stats: 131,790 msgs/sec ~ 2.01 MB/sec
|Pub stats: 148,775 msgs/sec ~ 2.27 MB/sec

|JetStream publication performance (memory)
|Pub stats: 156,362 msgs/sec ~ 2.39 MB/sec
|Pub stats: 119,144 msgs/sec ~ 1.82 MB/sec

|JetStream consumption (replay) performance (memory)
|Sub stats: 329,303 msgs/sec ~ 5.02 MB/se
|Sub stats: 360,377 msgs/sec ~ 5.50 MB/sec
|===

== Test2
Topology: hub: c5.2xlarge x2 leaf: c5.2xlarge  x2
|===
|test |A |B

|publisher throughput test(hub)
|Pub stats: 4,865,471 msgs/sec ~ 74.24 MB/sec ,
|Pub stats: 4,773,873 msgs/sec ~ 72.84 MB/sec

|publisher throughput test (spoke-1)
|Pub stats: 4,662,209 msgs/sec ~ 71.14 MB/sec
|Pub stats: 4,659,870 msgs/sec ~ 71.10 MB/sec

|publish/subscribe throughput test(hub->hub)
|NATS Pub/Sub stats: 4,441,848 msgs/sec ~ 67.78 MB/sec ,Pub stats: 2,339,093 msgs/sec ~ 35.69 MB/sec ,Sub stats: 2,264,055 msgs/sec ~ 34.55 MB/sec
|NATS Pub/Sub stats: 4,990,450 msgs/sec ~ 76.15 MB/sec ,Pub stats: 2,496,330 msgs/sec ~ 38.09 MB/sec ,Sub stats: 2,495,676 msgs/sec ~ 38.08 MB/sec

|publish/subscribe throughput test(hub->spoke-1)
| Sub stats: 1,851,831 msgs/sec ~ 226.05 MB/sec, Pub stats: 2,543,623 msgs/sec ~ 38.81 MB/sec
| Sub stats: 2,098,418 msgs/sec ~ 256.15 MB/sec, Pub stats: 2,124,810 msgs/sec ~ 32.42 MB/sec

|publish/subscribe throughput test(spoke-1->hub)
| Sub stats: 1,837,465 msgs/sec ~ 224.30 MB/sec, Pub stats: 2,061,719 msgs/sec ~ 31.46 MB/sec
| Sub stats: 1,973,527 msgs/sec ~ 240.91 MB/sec, Pub stats: 2,093,935 msgs/sec ~ 31.95 MB/sec

|request/reply latency test(hub->hub->hub)
|Pub stats: 24,426 msgs/sec ~ 381.66 KB/sec ,
|Pub stats: 84,878 msgs/sec ~ 1.30 MB/sec !!!  PUB 20->100

|request/reply latency test(spoke-1->hub->spoke-1)
|Pub stats: 20,892 msgs/sec ~ 326.44 KB/sec
|Pub stats: 79,143 msgs/sec ~ 1.21 MB/sec !!!  PUB 20->100

|JetStream publication performance (file)
|Pub stats: 246,737 msgs/sec ~ 3.76 MB/se
|Pub stats: 225,923 msgs/sec ~ 3.45 MB/sec

|JetStream publication performance (memory)
|Pub stats: 287,506 msgs/sec ~ 4.39 MB/sec
|Pub stats: 245,115 msgs/sec ~ 3.74 MB/sec

|JetStream consumption (replay) performance (memory)
|Sub stats: 1,159,138 msgs/sec ~ 17.69 MB/sec
|Sub stats: 1,057,679 msgs/sec ~ 16.14 MB/sec
|===


== Stream

[source]
----
Information for Stream ORDERS created 2022-04-08T13:11:05Z

Configuration:

             Subjects: ORDERS.*
     Acknowledgements: true
            Retention: File - Limits
             Replicas: 1
       Discard Policy: Old
     Duplicate Window: 2m0s
    Allows Msg Delete: true
         Allows Purge: true
       Allows Rollups: false
     Maximum Messages: unlimited
        Maximum Bytes: unlimited
          Maximum Age: unlimited
 Maximum Message Size: unlimited
    Maximum Consumers: unlimited


Cluster Information:

                 Name: nats
               Leader: cluster-nats-0

State:

             Messages: 500
                Bytes: 47 KiB
             FirstSeq: 1 @ 2022-04-08T13:11:05 UTC
              LastSeq: 500 @ 2022-04-08T13:11:05 UTC
     Active Consumers: 1
----